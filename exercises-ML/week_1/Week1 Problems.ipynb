{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYqXvqbTfqRC"
      },
      "source": [
        "# Weekly Tutorial Problems: Week 1\n",
        "Jérôme Dockès, Nikhil Bhagwat, Jacob Sanz-Robinson.\n",
        "\n",
        "Welcome to the first set of tutorial problems of the ABCD ReproNim Machine Learning course! They are largely based on Jérôme and Nikhil's exercises for the MAIN educational course: https://github.com/neurodatascience/main-2021-ml-parts-1-2\n",
        "\n",
        "Let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOp_R4uDfqRF"
      },
      "source": [
        "# Lecture 1: Core Concepts in Machine Learning 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi-cGBfafqRG"
      },
      "source": [
        "## Question 1: Fitting and Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3v7U7GFfqRG"
      },
      "source": [
        "We have a chunk of code below. It imports the necessary libraries, generates some random data, and creates an instance of a linear least squares model with l2 (ridge) regularization. It fits the model to the data, obtains predictions, and calculates the Mean Squared Error of the predictions that were made.\n",
        "\n",
        "**Your task:**\n",
        "* The code runs, but there is a methodological issue in the way we are fitting data. What is this issue in the code below?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZT3nJn6fqRH",
        "outputId": "6191e69d-4822-4eaf-81b3-e1c7590ea837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean Squared Error: 4.517304934456885e-18\n",
            "MSE is 0 up to machine precision: True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=80, n_features=600, noise=10, random_state=0) #Generates a random linear combination of random features, with noise.\n",
        "\n",
        "model = Ridge(alpha=1e-8)\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X)\n",
        "mse = mean_squared_error(y, predictions)\n",
        "\n",
        "print(f\"\\nMean Squared Error: {mse}\")\n",
        "print(\"MSE is 0 up to machine precision:\", np.allclose(mse, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vAvAeTLfqRI"
      },
      "source": [
        "Now let's compare training and testing performance. We generate some new data, and split it into a train and test set.\n",
        "\n",
        "**Your task:**\n",
        "* Fit the model on training data only, get predictions for test data, and compute prediction error. Is it a much larger than error on the training data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrzc5It8fqRI",
        "outputId": "7a0806ef-6661-491e-a681-c9919dbe2077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean Squared Error: 25420.842988621494\n"
          ]
        }
      ],
      "source": [
        "X, y = make_regression(n_samples=160, n_features=600, noise=10, random_state=0)\n",
        "X_train, X_test = X[:80], X[80:]\n",
        "y_train, y_test = y[:80], y[80:]\n",
        "\n",
        "model = Ridge(alpha=1e-8)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "\n",
        "print(f\"\\nMean Squared Error: {mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slpCS4cXfqRJ"
      },
      "source": [
        "## Question 2: Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l_DXmRgfqRK"
      },
      "source": [
        "As in the previous question, we import the libraries we need, generate some random data, and create an instance of a linear least squares model with l2 regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fe8EmePHfqRL"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "X, y = make_regression(noise=10) #Generates a random linear combination of random features, with noise.\n",
        "model = Ridge()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WL0ikJEfqRL"
      },
      "source": [
        "**Your task (should be ~2 lines of code):**\n",
        "* Using an appropriate function from scikit-learn, compute cross-validation scores for a ridge regression on this dataset.\n",
        "* What cross-validation strategy are you using? What do the scores represent -- what performance metric is used?\n",
        "* What is a good choice for k?\n",
        "* Once you are satisfied with the scores, fit the model to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "z8I4MWDjfqRM",
        "outputId": "64cc3bd4-5565-4e45-cbcd-095d92fe7941"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.006001  , 0.0036478 , 0.00277138, 0.00238705]),\n",
              " 'score_time': array([0.00188541, 0.00064993, 0.00060177, 0.00055599]),\n",
              " 'test_score': array([-10947.34163986,  -5259.53332867, -18832.16518549, -11221.27709501]),\n",
              " 'train_score': array([ -8.24343081, -10.97433602,  -7.87999792,  -8.47754822])}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ridge()"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "cv_results = model_selection.cross_validate(model, X, y, scoring=\"neg_mean_squared_error\", cv=model_selection.KFold(4),return_train_score=True)\n",
        "display(cv_results)\n",
        "model.fit(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djTCQLo8fqRN"
      },
      "source": [
        "## Question 3: Hyperparameters and Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCkg84yefqRN"
      },
      "source": [
        "The chunk of code below imports the necessary datasets, generates some random data for us to classify.\n",
        "\n",
        "**Your task:**\n",
        "* Modify the \"model\" variable. It should use the GridSearchCV to run a Logistic Regression, doing a grid search over at least 3 different values of C (I suggest they each vary by an order of magnitude), and \"l1\" and \"l2\" penalty types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "v2RgGgW9fqRN",
        "outputId": "593dbe12-d1d2-4b83-a867-0bc8d241ad6f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.13151264, 0.10724068, 0.10302949, 0.10636783, 0.10128164]),\n",
              " 'score_time': array([0.00074244, 0.00070357, 0.00070119, 0.00073504, 0.00066257]),\n",
              " 'test_score': array([0.8 , 0.85, 0.85, 0.75, 0.9 ])}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "X, y = make_classification() #Generate random data\n",
        "model = GridSearchCV(LogisticRegression(solver=\"liblinear\"),\n",
        "             param_grid={\"C\": [.1, 1, 10], \"penalty\": [\"l1\", \"l2\"]})\n",
        "cv_scores = cross_validate(model, X, y)\n",
        "display(cv_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNwBxCLbfqRP"
      },
      "source": [
        "## Question 4: Prediction and Performance Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XOTpc8MfqRP"
      },
      "source": [
        "In this question we're going to build a model that can recognize handwritten digits.\n",
        "\n",
        "Firstly, below are all the libraries we are going to use for this question. I suggest you Google them, or have a glance at their documentation if you don't already recognize them! Popular libraries often have very informative documentation, complete with examples.\n",
        "For example: https://scikit-learn.org/stable/user_guide.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Uv-5-QG7fqRP"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxUylzrJfqRP"
      },
      "source": [
        "We're going to download the dataset we'll be using. It's the MNIST dataset - a set of 70,000 images of digits handwritten by high school students. It's a commonly used benchmark for Machine Learning algorithms, and a part of many Machine Learning courses, earning it it's infamous nickname of the 'Hello World' of Machine Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4VRMfc8fqRQ",
        "outputId": "e7986737-82b4-4151-8138-1de178167cd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "mnist = fetch_openml('mnist_784', version=1, as_frame= False) #~130MB, might take a little time to download!\n",
        "mnist.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwBIQ_wJfqRQ"
      },
      "source": [
        "Let's look at the data dimensions, what the first digit looks like, and what it's label is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "aiwF5wXUfqRQ",
        "outputId": "acd00681-d664-4ec0-d1f1-f7ecb7cb802e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 784)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "print(X.shape) \n",
        "\n",
        "some_digit = X[0]\n",
        "some_digit_image = some_digit.reshape(28, 28)\n",
        "plt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9sDRy_ufqRS"
      },
      "source": [
        "Now we're going to separate the data to use the first 60,000 images as our training set, and the final 10,000 for our test set.\n",
        "\n",
        "We're going to try to quickly make a binary classification model. It will be able to tell the difference between the number 5 and all the other digits. A 5-detector.\n",
        "\n",
        "Our classifier will be an instance of a SGDClassifier model. I've chosen this classifier because it can handle large datasets efficiently. The default setting we are using is a linear SVM (you will see what this is next week). It is sped up by using a variation of Gradient Descent - Stochastic Gradient Descent (deals with training instances independently, one at a time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QxO5UTP9fqRS"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
        "y_train = y_train.astype(np.int8) #Casting labels from strings to integers\n",
        "\n",
        "#Here we are binarizing our labels. All labels that are 5 are converted to True, and the rest to False. \n",
        "y_train_5 = (y_train == 5)\n",
        "y_test_5 = (y_test == 5)\n",
        "\n",
        "sgd_clf = SGDClassifier(random_state=42) #42 is arbitrarily chosen. From documentation: \"Pass an int for reproducible output across multiple function calls\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct7XziVPfqRS"
      },
      "source": [
        "**Your task (should only be ~3 lines of code):**\n",
        "* Fit the sgd_clf model to the training data, and make a prediction using it. Was the prediction it made right?\n",
        "* Run cross validation using 3 folds on the model to determine it's accuracy. What accuracies are you getting for the 3 folds?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwXFSHIqfqRS",
        "outputId": "ed31fc45-c17a-47d4-8ff2-f9a37e6c8962"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.95035, 0.96035, 0.9604 ])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "sgd_clf.fit(X_train,y_train_5)\n",
        "sgd_clf.predict([some_digit])\n",
        "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89pPFee9fqRS"
      },
      "source": [
        "If you did everything right, you probably got something amazing, like 95% accuracy on all folds.\n",
        "Too good to be true? Yup...only about 10% of the images are 5s, so if you always guess that an image is NOT a 5, you will be right about 90% of the time...Not as impressive now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcyutr2vfqRT"
      },
      "source": [
        "**Your task (should only be ~2 lines of code):**\n",
        "\n",
        "Use the appropriate imported libraries to:\n",
        "* Obtain the predictions made on each fold of the training data.\n",
        "* Build a confusion matrix for these predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcFQmzVZfqRT",
        "outputId": "e9239428-8a63-459d-a2a6-d801fdde7876"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[53892,   687],\n",
              "       [ 1891,  3530]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
        "confusion_matrix(y_train_5, y_train_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_fW97pqfqRT"
      },
      "source": [
        "Remember that each row in a confusion matrix represents an actual class, while each column represents a predicted class. So the first row of this matrix considers non-5 images (the negative class). The second row considers the images of 5s (the positive class).\n",
        "\n",
        "**Your task:**\n",
        "In your confusion Matrix, how many did you get for each of\n",
        "* true negatives: \n",
        "* false positives: \n",
        "* false negatives: \n",
        "* true positives:\n",
        "\n",
        "* Use sklearn to find the precision, recall, and f1_score of these obtained predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noTdXgtAfqRT",
        "outputId": "e2e77da5-ea96-425a-cf26-734964e54299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8370879772350012\n",
            "0.6511713705958311\n",
            "0.7325171197343846\n"
          ]
        }
      ],
      "source": [
        "print(precision_score(y_train_5, y_train_pred))\n",
        "print(recall_score(y_train_5, y_train_pred))\n",
        "print(f1_score(y_train_5, y_train_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns1nmt_wfqRU"
      },
      "source": [
        "# Lecture 2: Core Concepts in Machine Learning 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xONbSXDwfqRU"
      },
      "source": [
        "## Question 1: Dataset Transformation\n",
        "**Your task:**\n",
        "Use an sklearn function seen in the recorded lectures to modify the variable X_scaled. X_scaled should standardize the feature variable X by removing the mean and scaling to unit variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfcMtx4wfqRU",
        "outputId": "5ce99a76-88d0-4b36-af15-344c26802edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:\n",
            "[[  0   1 -10]\n",
            " [  0  -1   0]\n",
            " [  1   0  10]\n",
            " [  1   0   0]]\n",
            "\n",
            "X scaled:\n",
            "[[-1.          1.41421356 -1.41421356]\n",
            " [-1.         -1.41421356  0.        ]\n",
            " [ 1.          0.          1.41421356]\n",
            " [ 1.          0.          0.        ]]\n",
            "\n",
            "mean: [0. 0. 0.]\n",
            "std: [1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = np.asarray([[0, 1, -10], [0, -1, 0], [1, 0, 10], [1, 0, 0]])\n",
        "print(f\"X:\\n{X}\\n\")\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "print(f\"X scaled:\\n{X_scaled}\\n\")\n",
        "print(f\"mean: {X_scaled.mean(axis=0)}\\nstd: {X_scaled.std(axis=0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urc7cW7XfqRV"
      },
      "source": [
        "## Question 2: Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuRxtFnYfqRV"
      },
      "source": [
        "As in the previous questions, we import relevant libraries and generate random data for regression. We then use the SelectKBest function from sklearn to perform feature selection (select features according to the k highest scores using f_regression - a linear model for testing the effect of a regressor), and perform cross validation using the Ridge linear model.\n",
        "\n",
        "**Your task:**\n",
        "What is the methodological issue with the chunk of code?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebmTbZdufqRV",
        "outputId": "090df68e-a67e-4dea-8e07-709bd1779243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature selection in 'preprocessing': [0.81169757 0.63046326 0.54143034 0.72676923 0.8752332 ]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "X, y = make_regression(noise=10, n_features=5000, random_state=0)\n",
        "\n",
        "X_reduced = SelectKBest(f_regression).fit_transform(X, y)\n",
        "scores = cross_validate(Ridge(), X_reduced, y)[\"test_score\"]\n",
        "print(\"feature selection in 'preprocessing':\", scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCjgfP1gfqRV"
      },
      "source": [
        "So...now let's fix it. To do this we are going to create a pipeline. We will make sure to fit the whole pipeline to the training set.\n",
        "\n",
        "**Your task:**\n",
        "* Modify the 'model' variable. Use the appropriate sklearn method to make a pipeline out of the functions from the previous chunk of code.\n",
        "* How do your results vary? Why do you think this is?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "N2gd19kFfqRW",
        "outputId": "a2862ac1-042e-483c-e86b-6d3d88cf56e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature selection on train set: [ 0.11577751  0.0439333  -0.27625968  0.32327364  0.28367254]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "model = make_pipeline(SelectKBest(f_regression), Ridge())\n",
        "scores_pipe = cross_validate(model, X, y)[\"test_score\"]\n",
        "print(\"feature selection on train set:\", scores_pipe)\n",
        "\n",
        "# Plotting our results!\n",
        "plt.boxplot(\n",
        "    [scores_pipe, scores],\n",
        "    vert=False,\n",
        "    labels=[\n",
        "        \"feature selection on train set\",\n",
        "        \"feature selection on whole data\",\n",
        "    ],\n",
        ")\n",
        "plt.gca().set_xlabel(\"R² score\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaBQ8tvvfqRW"
      },
      "source": [
        "## Question 3: Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWqtozUtfqRW"
      },
      "source": [
        "The sklearn library has included a dataset featuring breast cancer data. We fit the data, then transform it, standardizing the feature variable.\n",
        "\n",
        "**Your task:**\n",
        "* Use the sklearn PCA model to only keep the first two principal components of the data.\n",
        "* Fit the PCA model to breast cancer data, and tranform the data onto the first two principal components.\n",
        "* Compare original and reduced shape of the data...Is it behaving how you would expect?\n",
        "* Bonus: Create a plot to visualize the first vs second principal components (or any other method you would like to compare them)! Does it seem like the classes are easily separable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaGhVYWKfqRW",
        "outputId": "d2bcdb4e-d7fb-4f94-bf7f-778c5370c9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30)\n",
            "(569, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(cancer.data)\n",
        "X_scaled = scaler.transform(cancer.data)\n",
        "\n",
        "model = PCA(n_components=2)\n",
        "model.fit(X_scaled)\n",
        "X_model = model.transform(X_scaled)\n",
        "\n",
        "print( str(X_scaled.shape))\n",
        "print( str(X_model.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or7NjTlTfqRW"
      },
      "source": [
        "## Question 4: Big Picture - Scanning site prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zvEDnl_fqRX"
      },
      "source": [
        "We have some fMRI time series, that we use to compute a connectivity matrix for each participant. We use the connectivity matrix values as our input features to predict to which site the participant belongs.\n",
        "We classify participants using a logistic regression. However we make several additions.\n",
        "\n",
        "### Pipeline\n",
        "We use scikit-learn's `sklearn.pipeline.Pipeline`, that enables chaining several transformations into a single scikit-learn estimator (an object with a `fit` method). This avoids dealing with the connectivity feature extraction separately and ensures everything is fitted on the training data only -- which is crucial here because we will add scaling a dimensionality reduction step with Principal Component Analysis.\n",
        "\n",
        "### Scaling\n",
        "We add scaling of the input features using scikit-learn's StandardScaler, which removes the mean and scales the features to unit variance. This helps the logistic regression solver converge faster and often improves performance.\n",
        "\n",
        "### Dimensionality Reduction\n",
        "We also consider a pipeline that reduces the dimension of input features with PCA, and compare it to the baseline logistic regrssion. One advantage is that the pipeline that uses PCA can be fitted much faster.\n",
        "\n",
        "### Cross-validation\n",
        "Here, we will use scikit-learn's `cross_validate` to perform K-Fold cross-validation and get a better estimate of our model's generalization performance. This allows comparing logistic regression with and without PCA, as well as a naive baseline.\n",
        "\n",
        "Moreover, instead of the plain `LogisticRegression`, we use scikit-learn's `LogisticRegressionCV`, which automatically performs a nested cross-validation loop on the training data to select the best hyperparameter.\n",
        "\n",
        "**We therefore obtain a typical supervised learning experiment, with learning pipelines that involve chained transformations, hyperparameter selection, a cross-validation, and comparison of several models and a baseline.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwXEnfilfqRX"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "* Read, understand and run the code in the cell below. `load_connectivity_data` loads the data and returns the matrices `X` and `y`. `prepare_pipelines` returns a dictionary whose values are scikit-learn estimators and whose keys are names for each estimator. All estimators are instances of scikit-learn's `Pipeline`, and the first step is always connectivity feature extraction with nilearn's `ConnectivityMeasure`.\n",
        "\n",
        "\n",
        "* At the moment `prepare_pipelines` only returns 2 estimators: the logistic regression and a dummy estimator. Add a third estimator in the returned dictionary, which contains a dimensionality reduction step: a PCA with 20 components. To do so, add a `sklearn.decomposition.PCA` as the second step of the pipeline. Note 20 is an arbitrary choice; how could we set the number of components in a principled way? What is the largest number of components we could ask for?\n",
        "\n",
        "\n",
        "* There are 111 regions in the atlas we use to compute region-region connectivity matrices: the output of the `ConnectivityMeasure` has 111 * (111 - 1) / 2 = 6105 columns. If the dataset has 100 participants, What is the size of the coefficients of the logistic regression? of the selected (20 first) principal components? of the output of the PCA transformation (ie the compressed design matrix)?\n",
        "\n",
        "\n",
        "* Here we are storing data and model coefficients in arrays of 64-bit floating-point values, meaning each number takes 64 bits = 8 bytes of memory. Approximately how much memory is used by the design matrix X? by the dimensionality-reduced data (ie the kept left singular vectors of X)? by the principal components (the kept right singular vectors of X)?\n",
        "\n",
        "\n",
        "* As you can see, in this script we do not specify explicitly the metric functions that are used to evaluate models, but rely on scikit-learn's defaults instead. What metric is used in order to select the best hyperparameter? What metric is used to compute scores in `cross_validate`? Are these defaults appropriate for our particular situation?\n",
        "\n",
        "\n",
        "* We do not specify the cross-validation strategy either. Which cross-validation procedure is used in `cross_validate`, and by the `LogisticRegressionCV`? Are these choices appropriate?\n",
        "\n",
        "## Additional exercises (optional)\n",
        "\n",
        "* Try replacing the default metrics with other scoring functions from scikit-learn or functions that you write yourself. Does the relative performance of the models change?\n",
        "\n",
        "* Specify the cross-validation strategy explicitly, possibly choosing a different one than the default.\n",
        "\n",
        "* Add another estimator to the options returned by `prepare_pipelines`, that uses univariate feature selection instead of PCA.\n",
        "\n",
        "* What other approach could we use to obtain connectivity features of a lower dimension?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68cUvTHwfqRX"
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets\n",
        "from nilearn.connectome import ConnectivityMeasure\n",
        "\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P40UWYUfqRX"
      },
      "outputs": [],
      "source": [
        "def load_timeseries_and_site(n_subjects=100):\n",
        "    \"\"\"Load ABIDE timeseries and participants' site.\n",
        "    Returns X, a list with one array of shape (n_samples, n_rois) per\n",
        "    participant, and y, an array of length n_participants containing integers\n",
        "    representing the site each participant belongs to.\n",
        "    \"\"\"\n",
        "    data = datasets.fetch_abide_pcp(\n",
        "        n_subjects=n_subjects, derivatives=[\"rois_ho\"], quality_checked=False\n",
        "    )\n",
        "    X = data[\"rois_ho\"]\n",
        "    y = LabelEncoder().fit_transform(data[\"phenotypic\"][\"SITE_ID\"])\n",
        "    return X, y\n",
        "\n",
        "def prepare_pipelines():\n",
        "    \"\"\"Prepare scikit-learn pipelines for fmri classification with connectivity.\n",
        "    Returns a dictionary where each value is a scikit-learn estimator (a\n",
        "    `Pipeline`) and the corresponding key is a descriptive string for that\n",
        "    estimator.\n",
        "    As an exercise you need to add a pipeline that performs dimensionality\n",
        "    reduction with PCA.\n",
        "    \"\"\"\n",
        "    connectivity = ConnectivityMeasure(\n",
        "        kind=\"correlation\", vectorize=True, discard_diagonal=True\n",
        "    )\n",
        "    scaling = StandardScaler()\n",
        "    logreg = LogisticRegressionCV(solver=\"liblinear\", cv=3, Cs=3)\n",
        "    logistic_reg = make_pipeline(\n",
        "        clone(connectivity), clone(scaling), clone(logreg)\n",
        "    )\n",
        "    # make_pipeline is a convenient way to create a Pipeline by passing the\n",
        "    # steps as arguments. clone creates a copy of the input estimator, to avoid\n",
        "    # sharing the state of an estimator across pipelines.\n",
        "    dummy = make_pipeline(clone(connectivity), DummyClassifier())\n",
        "    # TODO: add a pipeline with a PCA dimensionality reduction step to this\n",
        "    # dictionary. You will need to import `sklearn.decomposition.PCA`.\n",
        "    return {\n",
        "        \"Logistic no PCA\": logistic_reg,\n",
        "        \"Dummy\": dummy,\n",
        "    }\n",
        "\n",
        "def compute_cv_scores(models, X, y):\n",
        "    \"\"\"Compute cross-validation scores for all models\n",
        "    `models` is a dictionary like the one returned by `prepare_pipelines`, ie\n",
        "    of the form `{\"model_name\": estimator}`, where `estimator` is a\n",
        "    scikit-learn estimator.\n",
        "    `X` and `y` are the design matrix and the outputs to predict.\n",
        "    Returns a `pd.DataFrame` with one row for each model and cross-validation\n",
        "    fold. Columns include `test_score` and `fit_time`.\n",
        "    \"\"\"\n",
        "    all_scores = []\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"Computing scores for model: '{model_name}'\")\n",
        "        model_scores = pd.DataFrame(cross_validate(model, X, y))\n",
        "        model_scores[\"model\"] = model_name\n",
        "        all_scores.append(model_scores)\n",
        "    all_scores = pd.concat(all_scores)\n",
        "    return all_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klwmvrCqfqRY"
      },
      "outputs": [],
      "source": [
        "X, y = load_timeseries_and_site()\n",
        "models = prepare_pipelines()\n",
        "all_scores = compute_cv_scores(models, X, y)\n",
        "print(all_scores.groupby(\"model\").mean())\n",
        "sns.stripplot(data=all_scores, x=\"test_score\", y=\"model\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "Week1 Problems.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}